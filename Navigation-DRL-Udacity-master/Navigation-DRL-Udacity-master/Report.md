# Navigation-DRL-Udacity Report
Udacity Deep Reinforcement Learning Nanodegree Program<br/>
P1-Navigation <br/>


### Description
For this project, I will train an agent to navigate (and collect bananas!) in a large, square world.<br/>
[Environment view](https://github.com/vahid-f/Deep-RL/blob/master/Navigation-DRL-Udacity-master/Navigation-DRL-Udacity-master/images/videoexample.mp4)



### Goal
A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.<br/>
The task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes.

### States and actions
The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction. <br/> Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:<br/>

0 - move forward.<br/>
1 - move backward.<br/>
2 - turn left.<br/>
3 - turn right.

### Environment
The used environment for Windows 64 :
[Banan Envoronment](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)


### Solution
I have solved the project by Deep Q-Networks and also considering the 'Experience Replay' and 'Fixed Q Targets' to improvement the training. You can find the solution by refering to [training code](https://github.com/vahid-f/Deep-RL/tree/master/Navigation-DRL-Udacity-master/Navigation-DRL-Udacity-master/Training%20code) and using below clarification. <br/>

**Navigation.py**: This is the main script which shows the intersections between the environment and agent by using following functions. <br/>

**model.py** : This is a neural network consists of two hidden layers with 64 nodes. I have also used Relu function as activation function.<br/>

**dqn_agent.py** : This is a class of agents activites which make agent be trained. <br/>
In this part, the agent will take samples in RelayBuffer and also will be trained for each 4 steps. Consider below used functions for more details:<br/>

**Agent class**: <br/>
> * step() : It stores a step taken by the agent in the Replay Buffer. Then the agent will be learnt Every 4 steps.
> * act(): This returns actions for the given state based on an Epsilon-greedy.
> * learn(): This update the Neural Network value parameters.
> * soft_update(): It updates the value from the target Neural Network from the local network weights (That's part of the Fixed Q Targets technique).

**ReplayBuffer class**:<br/>
It implements a fixed-size buffer to store experience tuples (state, action, reward, next_state, done)
> * add(): It appends the samples to the memory
> * sample(): It randomly samples a batch of experience steps in the memory.

**model_weights.pth**: This stores the trained weights. 

### used parameters
BUFFER_SIZE = int(1e5)   &nbsp; &nbsp; replay buffer size<br/>
BATCH_SIZE = 64         &nbsp; &nbsp; minibatch size<br/>
GAMMA = 0.99            &nbsp; &nbsp; discount factor<br/>
TAU = 1e-3              &nbsp; &nbsp; for soft update of target parameters<br/>
LR = 5e-4               &nbsp; &nbsp; learning rate <br/>
UPDATE_EVERY = 4        &nbsp; &nbsp; how often to update the network

### Result

<img src="https://github.com/vahid-f/Deep-RL/blob/master/Navigation-DRL-Udacity-master/Navigation-DRL-Udacity-master/images/scores.png" />


<img src="https://github.com/vahid-f/Deep-RL/blob/master/Navigation-DRL-Udacity-master/Navigation-DRL-Udacity-master/images/scoretrend.png" />


### Ideas for future work

As discussed in the Udacity Course, a further evolution to this project would be to train the agent directly from the environment's observed raw pixels instead of using the environment's internal states (37 dimensions).other algorithms instead of Deep Q-Networks, we can introduce enhancers like experience replay or tweak the parameters

